{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> This notebook contains the whole evaluation and analysis of DataProcessor, the newest InterSCity's microservice that is focused on abstracting cluster processing from smart city applications developers.\n",
    "\n",
    "\n",
    "# DataProcessor Report and Experiment\n",
    "\n",
    "To successfully schematize and evaluate DataProcessor, we prepared this notebook, that contains our code and analysis of an experiment that we defined to evaluate DataProcessor. The experiment focus on solving a bigger problem, which is broken down into smaller ones.\n",
    "\n",
    "Since we will compare two approaches, one that (1) uses DataProcessor and the other that (2) uses Spark directly, for convenience, we are going to name such approaches **DataProcessor Solutions** and **Raw-Spark Solutions**, although DataProcessor still uses Spark to process its data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The problem that we are going to solve is the following one:\n",
    "\n",
    "#### *How to identify anomalous traffic in a city, such as São Paulo?*####\n",
    "\n",
    "To solve the problem we:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**1.** Generate almost-real São Paulo traffic data using a simulator called SimDiasca [SimDiasca](https://github.com/DylanGuedes/sim-diasca-blue) that already has a city traffic model called [InterSCSimulator](https://github.com/DylanGuedes/interscsimulator-blue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The reason why we are going to use a simulator is because, at some point, we also need to evaluate streaming processing; if we choose to rely on the available real data (i.e: [olhovivo API](http://olhovivo.sptrans.com.br/)) it would not be possible to evaluate scenarios that generates Big Data quickly (olhovivo API only broadcast and update traffic data each minute, which is clearly not-so-fast). On the other hand, the simulation is able to generate data in a miliseconds fashion, as you may know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**2.** Use the [InterSCity Smart City Platform](https://gitlab.com/interscity/interscity-platform) to store the generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " InterSCity has a microservice called `DataCollector` that uses MongoDB to store its data. Therefore, to be able to extract InterSCity data, we are going to use a Mongo connector that integrates with Spark, named `MongoConnector`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**3.** Compare the required time to extract InterSCity data using both, DataProcessor and Raw-Spark solutions, comparing also LOC and knowledge-complexity of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**4.** Since our problem is based around classifying anomalous state in a city, we define that the first simulation (1st point) is the normal city state, and a second simulation will be an anomalous version of the first. Also by our definition, the anomalous city will be the first city without key edges in its graph.\n",
    "\n",
    "We defined key edges as the top100 most used edges by cars during a simulation. Without the key edges, we expect the city to have worse traffic, which will help in identifying anomalous edges.\n",
    "\n",
    "The process of couting how many cars walked in each edge will be done by Spark with both approaches, DataProcessor and Raw-Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**5.** Then, we will define a model that is capable of classifying city edges as anomalous or not. We are going to use a **ultra-simple-model** that just uses mean and stddev and will classify as anomalous any edge that is out of the range [mean-stddev, mean+stddev]. Again, we create both, DataProcessor and Raw-Spark solutions and evaluate both. At the end we will store this model as a file to be reopened later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**6.** We will start a data processor stream that receive smart city data and classify edges into anomalous or not. This one uses DataProcessor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**7.** We ran a new simulation that uses the anomalous graph (i.e: without key edges). In real time, the data points generated by this simulation will be classified by the stream started at the previous step. The anomalous datapoints will be stored to further use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**8.** Now, we repeat steps 6 and 7 but now using a Raw-Spark stream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**9.** Finally, we compare both stream solutions regarding its performance and usability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we know the steps, we can finally start the whole thing! First things first, we will configure the environment so that we can jump into step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Environment Setup \n",
    "\n",
    "We will start configuring spark parameters to correctly be able to interact with the Revoada cluster and DataProcessor data.\n",
    "\n",
    "The Master URL, as the name suggests, is the master's host. Since we are using Spark Standalone Cluster Manager, the URL starts with `spark://`, instead of `yarn://` or `mesos://`.\n",
    "\n",
    "Then, we fill SparkConf (Spark entity that controls Spark parameters) to also include in its dependencies the Mongo Connector. As you may know, InterSCity's Data Collector uses MongoDB to store its data; since we want to gather this data directly/bypassing, Mongo Connector is our best option.\n",
    "\n",
    "At the end we just instantiate a SparkContext and a SparkSession using the SparkConf that we just configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "MASTER_URL = \"spark://10.4.0.20:7077\"\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.eventLog.enabled\", \"true\")\n",
    "        .set(\"spark.history.fs.logDirectory\", \"/tmp/spark-events\")\n",
    "        .set(\"spark.app.name\", \"step2-datacollector-extraction\")\n",
    "        .set(\"spark.driver.memory\", \"6G\")\n",
    "        .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\") # mongo connector\n",
    "        .setMaster(MASTER_URL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DataCollector Data Gathering\n",
    "### 2.1. Raw-Spark solution\n",
    "\n",
    "With a correct Spark environment, we can finally interact with DataCollector to gather InterSCity's data. We will first list the Raw-Spark solution, explaining the key code used. The parameters used are:\n",
    "- `default_uri` is just the DataCollector MongoDB's host. \n",
    "- `data_collector_development` database name\n",
    "- `sensor_values` mongo collection\n",
    "- `27017`  mongo port\n",
    "\n",
    "Then, we define a `pipeline` to be used in MongoDB extraction. This parameter is important for heterogenous DataCollector environments: if the DataCollector has stored different capabilities, the pipeline filter out the capabilities that are not important for you, so that the query is executed faster.\n",
    "\n",
    "Right next we define the city traffic `schema` - the schema of the simulation traffic is defined.\n",
    "\n",
    "With all these things defined we can finally extract the data, which occur with the command `load`. The simulation only generates data points, that specify at which node a given car was at some timestamp.\n",
    "\n",
    "The DataFrame `df` contains the raw simulation data - you can test it, if you want.\n",
    "\n",
    "We choose to also store the dataframe as a parquet file because, as you can see, without it we only apply **transformations** to our dataframe. Since Spark uses lazy evaluation, without an **action** it will not run anything; since writing to a file is an action, it will trigger Spark processing so that we can finally measure its performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b4adab57fffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "import time\n",
    "\n",
    "from pyspark.sql.types import LongType, StringType, StructType\n",
    "\n",
    "capability = \"city_traffic\"\n",
    "default_uri = \"mongodb://10.4.0.20:27017/data_collector_development\"\n",
    "default_collection = \"sensor_values\"\n",
    "pipeline = \"{'$match': {'capability': '\"+capability+\"'}}\"\n",
    "\n",
    "sch = (StructType()\n",
    "    .add(\"nodeID\", LongType())\n",
    "    .add(\"tick\", LongType())\n",
    "    .add(\"uuid\", StringType()))\n",
    "\n",
    "df = (spark\n",
    "    .read\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "    .option(\"spark.mongodb.input.uri\", \"{0}.{1}\".format(default_uri, default_collection))\n",
    "    .option(\"pipeline\", pipeline)\n",
    "    .schema(sch)\n",
    "    .load()\n",
    "    .withColumnRenamed(\"nodeID\", \"U\")\n",
    "    .withColumnRenamed(\"tick\", \"T0\"))\n",
    "\n",
    "(df\n",
    "    .write\n",
    "    .format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/tmp/dataprocessor-report/step2.parquet\"))\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time diff: 1556044609.9297001 - 1556044800.4620075 = 190.53230738639832s\n"
     ]
    }
   ],
   "source": [
    "time_spent = t2-t0\n",
    "\n",
    "print(\"Time diff: {0} - {1} = {2}s\".format(t0, t2, time_spent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, just to compare, according to Python, the action lasted 190.53s, which is pretty close to SparkUI results:\n",
    "\n",
    "![RawSparkSummaryMetrics](task2-raw-spark/raw-spark-history-index.png)\n",
    "\n",
    "3.2min or 192 seconds, pretty close to the result gave by Python's `time` method. This comparison shows that using Python or Spark to measure the time spent into processing is not **that** relevant.\n",
    "\n",
    "Now, we should see the RDD-DAG that Spark defined for our little problem:\n",
    "![RawSparkStages](task2-raw-spark/raw-spark-stages.png)\n",
    "\n",
    "The entire problem was solved in a single stage, which means that no shuffle happened.\n",
    "\n",
    "Also, it is good to see a summary of the defined tasks:\n",
    "![RawSparkTasksSummary](task2-raw-spark/raw-spark-summary-metrics.png)\n",
    "\n",
    "The most relevant thing is the `task deserialization` and `tasks serialization` - the final amount is low which is great. However, since we are using Python, in more complex tasks these metrics will become relevant since the overhead of translating Python into JVM can turn into a real overhead.\n",
    "\n",
    "You can load all these results through file `app-20190423183649-0004`, stored in `task2-raw-spark` folder; you just need to copy it to the same folder that your Spark is configured to use as the log folder (in our case, we defined this folder to be `/tmp/spark-events` with the parameters `spark.eventLog.enabled=true` and `spark.history.fs.logDirectory=/tmp/spark-events`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. All available hardware resources with lower amount of data\n",
    "\n",
    "We are going to use now all available resources but with less fraction of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o376.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: GC overhead limit exceeded\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1171)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:1069)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1013)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0f563be2d4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         .save(\"/tmp/dataprocessor-report/step2.parquet\"))\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o376.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: GC overhead limit exceeded\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1171)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:1069)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1013)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "total = 33466742 \n",
    "\n",
    "samples = [str(int(total*0.01)), str(int(total*0.1)), str(int(total*0.5)), str(int(total*0.95))]\n",
    "processing_times = []\n",
    "\n",
    "for u in samples:\n",
    "    spark.stop()\n",
    "    sc.stop()\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "    import time\n",
    "\n",
    "    from pyspark.sql.types import LongType, StringType, StructType\n",
    "\n",
    "    capability = \"city_traffic\"\n",
    "    default_uri = \"mongodb://10.4.0.20:27017/data_collector_development\"\n",
    "    default_collection = \"sensor_values\"\n",
    "    pipeline = \"[{'$match': {'capability': '\"+capability+\"'}}, {'$limit': \"+u+\"}]\"\n",
    "\n",
    "    sch = (StructType()\n",
    "        .add(\"nodeID\", LongType())\n",
    "        .add(\"tick\", LongType())\n",
    "        .add(\"uuid\", StringType()))\n",
    "\n",
    "    df = (spark\n",
    "        .read\n",
    "        .format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "        .option(\"spark.mongodb.input.uri\", \"{0}.{1}\".format(default_uri, default_collection))\n",
    "        .option(\"pipeline\", pipeline)\n",
    "        .schema(sch)\n",
    "        .load()\n",
    "        .withColumnRenamed(\"nodeID\", \"U\")\n",
    "        .withColumnRenamed(\"tick\", \"T0\"))\n",
    "\n",
    "    (df\n",
    "        .write\n",
    "        .format(\"parquet\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(\"/tmp/dataprocessor-report/step2.parquet\"))\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    spark.stop()\n",
    "    sc.stop()\n",
    "    \n",
    "    processing_times.append(t2-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"jsonapi\":{\"version\":\"1.0\"},\"included\":[{\"type\":\"job-script\",\"id\":\"3\",\"attributes\":{\"title\":\"Extract Collector\",\"path\":\"collectorsource.py\",\"language\":\"python\",\"code-sample\":\"from pyspar\\nfrom pyspar\\nfrom pyspar\\nimport requ\\nimport os\\nfrom pyspar\\n\\nimport sys\\n\\nif __name__\\n    # Loadi\",\"code\":\"from pyspark.sql.types import StructType, StructField, ArrayType, StringType, DoubleType, IntegerType, DateType\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import explode, col\\nimport requests\\nimport os\\nfrom pyspark import SparkContext, SparkConf\\n\\nimport sys\\n\\nif __name__ == '__main__':\\n    # Loading the dataset\\n    my_uuid = str(sys.argv[1])\\n    os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\\n    url = \\\"http://localhost:4000\\\" + '/api/job_templates/{0}'.format(my_uuid)\\n    response = requests.get(url)\\n    params = response.json()[\\\"data\\\"][\\\"attributes\\\"][\\\"user-params\\\"]\\n\\n    functional_params = params[\\\"functional\\\"]\\n    capability = params[\\\"interscity\\\"][\\\"capability\\\"]\\n\\n    MASTER_URL = \\\"spark://10.4.0.20:7077\\\"\\n    conf = (SparkConf()\\n     .set(\\\"spark.eventLog.enabled\\\", \\\"true\\\")\\n     .set(\\\"spark.history.fs.logDirectory\\\", \\\"/tmp/spark-events\\\")\\n     .set(\\\"spark.app.name\\\", \\\"step2-datacollector-extraction(dataprocessor)\\\")\\n     .set(\\\"spark.jars.packages\\\", \\\"org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\\\")\\n     .setMaster(MASTER_URL))\\n    sc = SparkContext(conf=conf)\\n    spark = SparkSession(sc)\\n    spark.sparkContext.setLogLevel(\\\"INFO\\\")\\n\\n    DEFAULT_URI = \\\"mongodb://10.4.0.20:27017/data_collector_development\\\"\\n    DEFAULT_COLLECTION = \\\"sensor_values\\\"\\n    pipeline = \\\"{'$match': {'capability': '\\\"+capability+\\\"'}}\\\"\\n    schema_params = params[\\\"schema\\\"]\\n    sch = StructType()\\n    sch.add(\\\"uuid\\\", StringType())\\n    for k,v in schema_params.items():\\n        if (k==\\\"string\\\"):\\n            sch.add(k, StringType())\\n        elif (k==\\\"double\\\"):\\n            sch.add(k, DoubleType())\\n        elif (k==\\\"integer\\\"):\\n            sch.add(k, LongType())\\n    df = (spark\\n            .read\\n            .format(\\\"com.mongodb.spark.sql.DefaultSource\\\")\\n            .option(\\\"spark.mongodb.input.uri\\\", \\\"{0}.{1}\\\".format(DEFAULT_URI, DEFAULT_COLLECTION))\\n            .option(\\\"pipeline\\\", pipeline)\\n            .schema(sch)\\n            .load())\\n    publish_strategy = response.json()[\\\"data\\\"][\\\"attributes\\\"][\\\"publish-strategy\\\"]\\n    file_path = publish_strategy[\\\"path\\\"]\\n    (df\\n     .write\\n     .format(publish_strategy[\\\"format\\\"])\\n     .mode(\\\"overwrite\\\")\\n     .save(\\\"/tmp/\\\" + file_path))\\n    spark.stop()\\n\"}},{\"type\":\"job-template\",\"relationships\":{\"job-script\":{\"data\":{\"type\":\"job-script\",\"id\":\"3\"}}},\"id\":\"3\",\"attributes\":{\"user-params\":{\"schema\":{\"uuid\":\"string\",\"tick\":\"integer\",\"nodeID\":\"integer\"},\"interscity\":{\"capability\":\"city_traffic\"},\"functional\":{}},\"title\":\"InterSCity Collector Data Gatherer\\n\",\"publish-strategy\":{\"path\":\"23-4-2019-14-55-18\",\"format\":\"csv\"}}}],\"data\":{\"type\":\"processing-job\",\"relationships\":{\"job-template\":{\"data\":{\"type\":\"job-template\",\"id\":\"3\"}}},\"id\":\"5\",\"attributes\":{\"uuid\":\"8fc45da8-a219-48d7-b359-c49abfafd5a5\",\"log\":\"Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\\nThe jars for the packages stored in: /home/ubuntu/.ivy2/jars\\n:: loading settings :: url = jar:file:/home/ubuntu/spark-2.4.1-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\\norg.mongodb.spark#mongo-spark-connector_2.11 added as a dependency\\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-9218e791-6ecf-41e7-b5b3-3e0e622f6f67;1.0\\n\\tconfs: [default]\\n\\tfound org.mongodb.spark#mongo-spark-connector_2.11;2.3.1 in central\\n\\tfound org.mongodb#mongo-java-driver;3.8.2 in central\\n:: resolution report :: resolve 302ms :: artifacts dl 6ms\\n\\t:: modules in use:\\n\\torg.mongodb#mongo-java-driver;3.8.2 from central in [default]\\n\\torg.mongodb.spark#mongo-spark-connector_2.11;2.3.1 from central in [default]\\n\\t---------------------------------------------------------------------\\n\\t|                  |            modules            ||   artifacts   |\\n\\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\\n\\t---------------------------------------------------------------------\\n\\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\\n\\t---------------------------------------------------------------------\\n:: retrieving :: org.apache.spark#spark-submit-parent-9218e791-6ecf-41e7-b5b3-3e0e622f6f67\\n\\tconfs: [default]\\n\\t0 artifacts copied, 2 already retrieved (0kB/7ms)\\n19/04/23 19:53:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\\n19/04/23 19:53:48 INFO SparkContext: Running Spark version 2.4.1\\n19/04/23 19:53:48 INFO SparkContext: Submitted application: step2-datacollector-extraction(dataprocessor)\\n19/04/23 19:53:48 INFO SecurityManager: Changing view acls to: ubuntu\\n19/04/23 19:53:48 INFO SecurityManager: Changing modify acls to: ubuntu\\n19/04/23 19:53:48 INFO SecurityManager: Changing view acls groups to: \\n19/04/23 19:53:48 INFO SecurityManager: Changing modify acls groups to: \\n19/04/23 19:53:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\\n19/04/23 19:53:49 INFO Utils: Successfully started service 'sparkDriver' on port 33040.\\n19/04/23 19:53:49 INFO SparkEnv: Registering MapOutputTracker\\n19/04/23 19:53:49 INFO SparkEnv: Registering BlockManagerMaster\\n19/04/23 19:53:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\\n19/04/23 19:53:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\\n19/04/23 19:53:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c64480d8-4cd9-46e4-87f3-9710f6adbdff\\n19/04/23 19:53:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\\n19/04/23 19:53:49 INFO SparkEnv: Registering OutputCommitCoordinator\\n19/04/23 19:53:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.\\n19/04/23 19:53:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://node-master:4040\\n19/04/23 19:53:49 INFO SparkContext: Added JAR file:///home/ubuntu/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.3.1.jar at spark://node-master:33040/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.3.1.jar with timestamp 1556049229700\\n19/04/23 19:53:49 INFO SparkContext: Added JAR file:///home/ubuntu/.ivy2/jars/org.mongodb_mongo-java-driver-3.8.2.jar at spark://node-master:33040/jars/org.mongodb_mongo-java-driver-3.8.2.jar with timestamp 1556049229701\\n19/04/23 19:53:49 INFO SparkContext: Added file file:///home/ubuntu/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.3.1.jar at spark://node-master:33040/files/org.mongodb.spark_mongo-spark-connector_2.11-2.3.1.jar with timestamp 1556049229724\\n19/04/23 19:53:49 INFO Utils: Copying /home/ubuntu/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.3.1.jar to /tmp/spark-6e79d75d-52b4-4095-97d0-0958e8ef7d86/userFiles-f8225b9d-b0ac-4ea0-94c1-bfa5ce2bff26/org.mongodb.spark_mongo-spark-connector_2.11-2.3.1.jar\\n19/04/23 19:53:49 INFO SparkContext: Added file file:///home/ubuntu/.ivy2/jars/org.mongodb_mongo-java-driver-3.8.2.jar at spark://node-master:33040/files/org.mongodb_mongo-java-driver-3.8.2.jar with timestamp 1556049229758\\n19/04/23 19:53:49 INFO Utils: Copying /home/ubuntu/.ivy2/jars/org.mongodb_mongo-java-driver-3.8.2.jar to /tmp/spark-6e79d75d-52b4-4095-97d0-0958e8ef7d86/userFiles-f8225b9d-b0ac-4ea0-94c1-bfa5ce2bff26/org.mongodb_mongo-java-driver-3.8.2.jar\\n19/04/23 19:53:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://10.4.0.20:7077...\\n19/04/23 19:53:49 INFO TransportClientFactory: Successfully created connection to /10.4.0.20:7077 after 43 ms (0 ms spent in bootstraps)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20190423195350-0013\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/0 on worker-20190423162829-10.4.0.71-39495 (10.4.0.71:39495) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/0 on hostPort 10.4.0.71:39495 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/1 on worker-20190423162828-10.4.0.169-33865 (10.4.0.169:33865) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/1 on hostPort 10.4.0.169:33865 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/2 on worker-20190423162829-10.4.1.3-39407 (10.4.1.3:39407) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/2 on hostPort 10.4.1.3:39407 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/3 on worker-20190423162821-10.4.0.89-38297 (10.4.0.89:38297) with 4 core(s)\\n19/04/23 19:53:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36240.\\n19/04/23 19:53:50 INFO NettyBlockTransferService: Server created on node-master:36240\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/3 on hostPort 10.4.0.89:38297 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/4 on worker-20190423162829-10.4.0.51-44785 (10.4.0.51:44785) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/4 on hostPort 10.4.0.51:44785 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/5 on worker-20190423162829-10.4.0.142-43097 (10.4.0.142:43097) with 4 core(s)\\n19/04/23 19:53:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/5 on hostPort 10.4.0.142:43097 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/6 on worker-20190423163108-10.4.0.240-41848 (10.4.0.240:41848) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/6 on hostPort 10.4.0.240:41848 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/7 on worker-20190423162829-10.4.0.25-35122 (10.4.0.25:35122) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/7 on hostPort 10.4.0.25:35122 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/8 on worker-20190423162828-10.4.0.55-43163 (10.4.0.55:43163) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/8 on hostPort 10.4.0.55:43163 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/9 on worker-20190423162829-10.4.0.141-33964 (10.4.0.141:33964) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/9 on hostPort 10.4.0.141:33964 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/10 on worker-20190423162829-10.4.0.97-41187 (10.4.0.97:41187) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/10 on hostPort 10.4.0.97:41187 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/11 on worker-20190423170951-10.4.0.12-36548 (10.4.0.12:36548) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/11 on hostPort 10.4.0.12:36548 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/12 on worker-20190423162829-10.4.0.18-42865 (10.4.0.18:42865) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/12 on hostPort 10.4.0.18:42865 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/13 on worker-20190423162828-10.4.0.193-37780 (10.4.0.193:37780) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/13 on hostPort 10.4.0.193:37780 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190423195350-0013/14 on worker-20190423162828-10.4.0.69-45836 (10.4.0.69:45836) with 4 core(s)\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20190423195350-0013/14 on hostPort 10.4.0.69:45836 with 4 core(s), 1024.0 MB RAM\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/4 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/0 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/9 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/13 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/6 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/1 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/10 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/7 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/3 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/12 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/5 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/2 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/8 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/11 is now RUNNING\\n19/04/23 19:53:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190423195350-0013/14 is now RUNNING\\n19/04/23 19:53:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, node-master, 36240, None)\\n19/04/23 19:53:50 INFO BlockManagerMasterEndpoint: Registering block manager node-master:36240 with 366.3 MB RAM, BlockManagerId(driver, node-master, 36240, None)\\n19/04/23 19:53:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, node-master, 36240, None)\\n19/04/23 19:53:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, node-master, 36240, None)\\n19/04/23 19:53:50 INFO EventLoggingListener: Logging events to file:/tmp/spark-events/app-20190423195350-0013\\n19/04/23 19:53:50 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\\n19/04/23 19:53:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/ubuntu/data-processor-backend/spark-warehouse').\\n19/04/23 19:53:50 INFO SharedState: Warehouse path is 'file:/home/ubuntu/data-processor-backend/spark-warehouse'.\\n19/04/23 19:53:51 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\\n19/04/23 19:53:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 280.0 B, free 366.3 MB)\\n19/04/23 19:53:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 433.0 B, free 366.3 MB)\\n19/04/23 19:53:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on node-master:36240 (size: 433.0 B, free: 366.3 MB)\\n19/04/23 19:53:52 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:539\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.1.3:33988) with ID 2\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.169:40982) with ID 1\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.18:51732) with ID 12\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.12:42210) with ID 11\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.240:41496) with ID 6\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.97:58304) with ID 10\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.25:55182) with ID 7\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.69:56152) with ID 14\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.55:35336) with ID 8\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.141:58322) with ID 9\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.89:40828) with ID 3\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.193:42536) with ID 13\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.142:55708) with ID 5\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.71:44482) with ID 0\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.1.3:42199 with 366.3 MB RAM, BlockManagerId(2, 10.4.1.3, 42199, None)\\n19/04/23 19:53:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.4.0.51:34680) with ID 4\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.169:41614 with 366.3 MB RAM, BlockManagerId(1, 10.4.0.169, 41614, None)\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.18:39237 with 366.3 MB RAM, BlockManagerId(12, 10.4.0.18, 39237, None)\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.240:39010 with 366.3 MB RAM, BlockManagerId(6, 10.4.0.240, 39010, None)\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.12:34310 with 366.3 MB RAM, BlockManagerId(11, 10.4.0.12, 34310, None)\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.25:35936 with 366.3 MB RAM, BlockManagerId(7, 10.4.0.25, 35936, None)\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.97:33732 with 366.3 MB RAM, BlockManagerId(10, 10.4.0.97, 33732, None)\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.69:39959 with 366.3 MB RAM, BlockManagerId(14, 10.4.0.69, 39959, None)\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.55:41483 with 366.3 MB RAM, BlockManagerId(8, 10.4.0.55, 41483, None)\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.71:43226 with 366.3 MB RAM, BlockManagerId(0, 10.4.0.71, 43226, None)\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.89:41688 with 366.3 MB RAM, BlockManagerId(3, 10.4.0.89, 41688, None)\\n19/04/23 19:53:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.141:41235 with 366.3 MB RAM, BlockManagerId(9, 10.4.0.141, 41235, None)\\n19/04/23 19:53:53 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.142:33654 with 366.3 MB RAM, BlockManagerId(5, 10.4.0.142, 33654, None)\\n19/04/23 19:53:53 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.193:36106 with 366.3 MB RAM, BlockManagerId(13, 10.4.0.193, 36106, None)\\n19/04/23 19:53:53 INFO BlockManagerMasterEndpoint: Registering block manager 10.4.0.51:38018 with 366.3 MB RAM, BlockManagerId(4, 10.4.0.51, 38018, None)\\n19/04/23 19:53:54 INFO MongoRelation: requiredColumns: uuid, filters: \\n19/04/23 19:53:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\\n19/04/23 19:53:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\\n19/04/23 19:53:55 INFO CodeGenerator: Code generated in 278.717705 ms\\n19/04/23 19:53:56 INFO cluster: Cluster created with settings {hosts=[10.4.0.20:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms', maxWaitQueueSize=500}\\n19/04/23 19:53:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\\n19/04/23 19:53:56 INFO connection: Opened connection [connectionId{localValue:1, serverValue:1110}] to 10.4.0.20:27017\\n19/04/23 19:53:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.4.0.20:27017, type=STANDALONE, state=CONNECTED, ok=true, version=ServerVersion{versionList=[4, 0, 8]}, minWireVersion=0, maxWireVersion=7, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4137075}\\n19/04/23 19:53:56 INFO MongoClientCache: Creating MongoClient: [10.4.0.20:27017]\\n19/04/23 19:53:56 INFO connection: Opened connection [connectionId{localValue:2, serverValue:1111}] to 10.4.0.20:27017\\n19/04/23 19:56:19 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\\n19/04/23 19:56:19 INFO DAGScheduler: Got job 0 (save at NativeMethodAccessorImpl.java:0) with 107 output partitions\\n19/04/23 19:56:19 INFO DAGScheduler: Final stage: ResultStage 0 (save at NativeMethodAccessorImpl.java:0)\\n19/04/23 19:56:19 INFO DAGScheduler: Parents of final stage: List()\\n19/04/23 19:56:19 INFO DAGScheduler: Missing parents: List()\\n19/04/23 19:56:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\\n19/04/23 19:56:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 151.3 KB, free 366.2 MB)\\n19/04/23 19:56:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 54.9 KB, free 366.1 MB)\\n19/04/23 19:56:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on node-master:36240 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161\\n19/04/23 19:56:19 INFO DAGScheduler: Submitting 107 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\\n19/04/23 19:56:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 107 tasks\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.4.0.69, executor 14, partition 0, ANY, 8032 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.4.0.18, executor 12, partition 1, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 10.4.0.141, executor 9, partition 2, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, 10.4.0.240, executor 6, partition 3, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, 10.4.0.97, executor 10, partition 4, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, 10.4.0.142, executor 5, partition 5, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, 10.4.0.55, executor 8, partition 6, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, 10.4.0.12, executor 11, partition 7, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, 10.4.0.169, executor 1, partition 8, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, 10.4.0.51, executor 4, partition 9, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, 10.4.1.3, executor 2, partition 10, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, 10.4.0.89, executor 3, partition 11, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, 10.4.0.193, executor 13, partition 12, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, 10.4.0.71, executor 0, partition 13, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, 10.4.0.25, executor 7, partition 14, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, 10.4.0.69, executor 14, partition 15, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16, 10.4.0.18, executor 12, partition 16, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17, 10.4.0.141, executor 9, partition 17, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18, 10.4.0.240, executor 6, partition 18, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19, 10.4.0.97, executor 10, partition 19, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20, 10.4.0.142, executor 5, partition 20, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21, 10.4.0.55, executor 8, partition 21, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22, 10.4.0.12, executor 11, partition 22, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23, 10.4.0.169, executor 1, partition 23, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24, 10.4.0.51, executor 4, partition 24, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25, 10.4.1.3, executor 2, partition 25, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26, 10.4.0.89, executor 3, partition 26, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27, 10.4.0.193, executor 13, partition 27, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28, 10.4.0.71, executor 0, partition 28, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29, 10.4.0.25, executor 7, partition 29, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30, 10.4.0.69, executor 14, partition 30, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31, 10.4.0.18, executor 12, partition 31, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32, 10.4.0.141, executor 9, partition 32, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33, 10.4.0.240, executor 6, partition 33, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34, 10.4.0.97, executor 10, partition 34, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35, 10.4.0.142, executor 5, partition 35, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36, 10.4.0.55, executor 8, partition 36, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37, 10.4.0.12, executor 11, partition 37, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38, 10.4.0.169, executor 1, partition 38, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39, 10.4.0.51, executor 4, partition 39, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, 10.4.1.3, executor 2, partition 40, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41, 10.4.0.89, executor 3, partition 41, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42, 10.4.0.193, executor 13, partition 42, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43, 10.4.0.71, executor 0, partition 43, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44, 10.4.0.25, executor 7, partition 44, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45, 10.4.0.69, executor 14, partition 45, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46, 10.4.0.18, executor 12, partition 46, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47, 10.4.0.141, executor 9, partition 47, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48, 10.4.0.240, executor 6, partition 48, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49, 10.4.0.97, executor 10, partition 49, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50, 10.4.0.142, executor 5, partition 50, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51, 10.4.0.55, executor 8, partition 51, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52, 10.4.0.12, executor 11, partition 52, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53, 10.4.0.169, executor 1, partition 53, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54, 10.4.0.51, executor 4, partition 54, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55, 10.4.1.3, executor 2, partition 55, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56, 10.4.0.89, executor 3, partition 56, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57, 10.4.0.193, executor 13, partition 57, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58, 10.4.0.71, executor 0, partition 58, ANY, 8050 bytes)\\n19/04/23 19:56:19 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59, 10.4.0.25, executor 7, partition 59, ANY, 8050 bytes)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.69:39959 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.18:39237 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.169:41614 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.1.3:42199 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.97:33732 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.71:43226 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.55:41483 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.12:34310 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.240:39010 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.51:38018 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.89:41688 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.25:35936 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.193:36106 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.141:41235 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.4.0.142:33654 (size: 54.9 KB, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.69:39959 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.1.3:42199 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.97:33732 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.71:43226 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.169:41614 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.12:34310 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.25:35936 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.18:39237 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.240:39010 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.55:41483 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.89:41688 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.142:33654 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.51:38018 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.141:41235 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.4.0.193:36106 (size: 433.0 B, free: 366.2 MB)\\n19/04/23 19:56:23 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60, 10.4.0.69, executor 14, partition 60, ANY, 8050 bytes)\\n19/04/23 19:56:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3422 ms on 10.4.0.69 (executor 14) (1/107)\\n19/04/23 19:56:24 INFO MongoClientCache: Closing MongoClient: [10.4.0.20:27017]\\n19/04/23 19:56:24 INFO connection: Closed connection [connectionId{localValue:2, serverValue:1111}] to 10.4.0.20:27017 because the pool has been closed.\\n19/04/23 19:56:30 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61, 10.4.0.89, executor 3, partition 61, ANY, 8050 bytes)\\n19/04/23 19:56:30 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 10430 ms on 10.4.0.89 (executor 3) (2/107)\\n19/04/23 19:56:31 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62, 10.4.0.169, executor 1, partition 62, ANY, 8050 bytes)\\n19/04/23 19:56:31 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 11194 ms on 10.4.0.169 (executor 1) (3/107)\\n19/04/23 19:56:31 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63, 10.4.0.25, executor 7, partition 63, ANY, 8050 bytes)\\n19/04/23 19:56:31 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 12104 ms on 10.4.0.25 (executor 7) (4/107)\\n19/04/23 19:56:32 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64, 10.4.0.69, executor 14, partition 64, ANY, 8050 bytes)\\n19/04/23 19:56:32 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 12224 ms on 10.4.0.69 (executor 14) (5/107)\\n19/04/23 19:56:32 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65, 10.4.0.142, executor 5, partition 65, ANY, 8050 bytes)\\n19/04/23 19:56:32 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 12343 ms on 10.4.0.142 (executor 5) (6/107)\\n19/04/23 19:56:32 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66, 10.4.0.71, executor 0, partition 66, ANY, 8050 bytes)\\n19/04/23 19:56:32 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 12957 ms on 10.4.0.71 (executor 0) (7/107)\\n19/04/23 19:56:33 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67, 10.4.0.69, executor 14, partition 67, ANY, 8050 bytes)\\n19/04/23 19:56:33 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 10031 ms on 10.4.0.69 (executor 14) (8/107)\\n19/04/23 19:56:33 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68, 10.4.0.142, executor 5, partition 68, ANY, 8050 bytes)\\n19/04/23 19:56:33 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 14077 ms on 10.4.0.142 (executor 5) (9/107)\\n19/04/23 19:56:35 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69, 10.4.0.89, executor 3, partition 69, ANY, 8050 bytes)\\n19/04/23 19:56:35 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 15976 ms on 10.4.0.89 (executor 3) (10/107)\\n19/04/23 19:56:35 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70, 10.4.0.69, executor 14, partition 70, ANY, 8050 bytes)\\n19/04/23 19:56:35 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 2707 ms on 10.4.0.69 (executor 14) (11/107)\\n19/04/23 19:56:36 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71, 10.4.0.193, executor 13, partition 71, ANY, 8050 bytes)\\n19/04/23 19:56:36 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 16336 ms on 10.4.0.193 (executor 13) (12/107)\\n19/04/23 19:56:36 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72, 10.4.0.89, executor 3, partition 72, ANY, 8050 bytes)\\n19/04/23 19:56:36 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 16374 ms on 10.4.0.89 (executor 3) (13/107)\\n19/04/23 19:56:37 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73, 10.4.0.69, executor 14, partition 73, ANY, 8050 bytes)\\n19/04/23 19:56:37 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 17180 ms on 10.4.0.69 (executor 14) (14/107)\\n19/04/23 19:56:37 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74, 10.4.0.142, executor 5, partition 74, ANY, 8050 bytes)\\n19/04/23 19:56:37 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 17849 ms on 10.4.0.142 (executor 5) (15/107)\\n19/04/23 19:56:38 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75, 10.4.0.71, executor 0, partition 75, ANY, 8050 bytes)\\n19/04/23 19:56:38 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 18595 ms on 10.4.0.71 (executor 0) (16/107)\\n19/04/23 19:56:38 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76, 10.4.1.3, executor 2, partition 76, ANY, 8050 bytes)\\n19/04/23 19:56:38 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 19051 ms on 10.4.1.3 (executor 2) (17/107)\\n19/04/23 19:56:39 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77, 10.4.0.69, executor 14, partition 77, ANY, 8050 bytes)\\n19/04/23 19:56:39 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 19424 ms on 10.4.0.69 (executor 14) (18/107)\\n19/04/23 19:56:39 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78, 10.4.0.25, executor 7, partition 78, ANY, 8050 bytes)\\n19/04/23 19:56:39 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 19649 ms on 10.4.0.25 (executor 7) (19/107)\\n19/04/23 19:56:39 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79, 10.4.1.3, executor 2, partition 79, ANY, 8050 bytes)\\n19/04/23 19:56:39 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 19668 ms on 10.4.1.3 (executor 2) (20/107)\\n19/04/23 19:56:39 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80, 10.4.0.141, executor 9, partition 80, ANY, 8050 bytes)\\n19/04/23 19:56:39 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 19684 ms on 10.4.0.141 (executor 9) (21/107)\\n19/04/23 19:56:39 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81, 10.4.0.89, executor 3, partition 81, ANY, 8050 bytes)\\n19/04/23 19:56:39 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 19829 ms on 10.4.0.89 (executor 3) (22/107)\\n19/04/23 19:56:39 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82, 10.4.0.18, executor 12, partition 82, ANY, 8050 bytes)\\n19/04/23 19:56:39 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 19924 ms on 10.4.0.18 (executor 12) (23/107)\\n19/04/23 19:56:39 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83, 10.4.0.89, executor 3, partition 83, ANY, 8050 bytes)\\n19/04/23 19:56:39 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 9536 ms on 10.4.0.89 (executor 3) (24/107)\\n19/04/23 19:56:39 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84, 10.4.0.18, executor 12, partition 84, ANY, 8050 bytes)\\n19/04/23 19:56:39 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 20051 ms on 10.4.0.18 (executor 12) (25/107)\\n19/04/23 19:56:39 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85, 10.4.0.142, executor 5, partition 85, ANY, 8050 bytes)\\n19/04/23 19:56:39 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 20078 ms on 10.4.0.142 (executor 5) (26/107)\\n19/04/23 19:56:40 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86, 10.4.0.142, executor 5, partition 86, ANY, 8050 bytes)\\n19/04/23 19:56:40 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 7889 ms on 10.4.0.142 (executor 5) (27/107)\\n19/04/23 19:56:40 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87, 10.4.0.12, executor 11, partition 87, ANY, 8050 bytes)\\n19/04/23 19:56:40 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 20647 ms on 10.4.0.12 (executor 11) (28/107)\\n19/04/23 19:56:40 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88, 10.4.1.3, executor 2, partition 88, ANY, 8050 bytes)\\n19/04/23 19:56:40 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 20715 ms on 10.4.1.3 (executor 2) (29/107)\\n19/04/23 19:56:40 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89, 10.4.0.51, executor 4, partition 89, ANY, 8050 bytes)\\n19/04/23 19:56:40 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 20883 ms on 10.4.0.51 (executor 4) (30/107)\\n19/04/23 19:56:40 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90, 10.4.0.141, executor 9, partition 90, ANY, 8050 bytes)\\n19/04/23 19:56:40 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 20897 ms on 10.4.0.141 (executor 9) (31/107)\\n19/04/23 19:56:40 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91, 10.4.0.25, executor 7, partition 91, ANY, 8050 bytes)\\n19/04/23 19:56:40 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 21068 ms on 10.4.0.25 (executor 7) (32/107)\\n19/04/23 19:56:41 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92, 10.4.0.169, executor 1, partition 92, ANY, 8050 bytes)\\n19/04/23 19:56:41 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 21231 ms on 10.4.0.169 (executor 1) (33/107)\\n19/04/23 19:56:41 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93, 10.4.0.142, executor 5, partition 93, ANY, 8050 bytes)\\n19/04/23 19:56:41 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 3416 ms on 10.4.0.142 (executor 5) (34/107)\\n19/04/23 19:56:41 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94, 10.4.0.141, executor 9, partition 94, ANY, 8050 bytes)\\n19/04/23 19:56:41 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 21670 ms on 10.4.0.141 (executor 9) (35/107)\\n19/04/23 19:56:41 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95, 10.4.0.97, executor 10, partition 95, ANY, 8050 bytes)\\n19/04/23 19:56:41 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96, 10.4.0.97, executor 10, partition 96, ANY, 8050 bytes)\\n19/04/23 19:56:41 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 21887 ms on 10.4.0.97 (executor 10) (36/107)\\n19/04/23 19:56:41 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 21881 ms on 10.4.0.97 (executor 10) (37/107)\\n19/04/23 19:56:42 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97, 10.4.0.142, executor 5, partition 97, ANY, 8050 bytes)\\n19/04/23 19:56:42 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 8514 ms on 10.4.0.142 (executor 5) (38/107)\\n19/04/23 19:56:42 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98, 10.4.0.51, executor 4, partition 98, ANY, 8050 bytes)\\n19/04/23 19:56:42 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 22630 ms on 10.4.0.51 (executor 4) (39/107)\\n19/04/23 19:56:42 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99, 10.4.0.240, executor 6, partition 99, ANY, 8050 bytes)\\n19/04/23 19:56:42 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 22780 ms on 10.4.0.240 (executor 6) (40/107)\\n19/04/23 19:56:42 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100, 10.4.0.97, executor 10, partition 100, ANY, 8050 bytes)\\n19/04/23 19:56:42 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 22861 ms on 10.4.0.97 (executor 10) (41/107)\\n19/04/23 19:56:42 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101, 10.4.0.25, executor 7, partition 101, ANY, 8050 bytes)\\n19/04/23 19:56:42 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 22984 ms on 10.4.0.25 (executor 7) (42/107)\\n19/04/23 19:56:42 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102, 10.4.0.51, executor 4, partition 102, ANY, 8050 bytes)\\n19/04/23 19:56:42 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 23106 ms on 10.4.0.51 (executor 4) (43/107)\\n19/04/23 19:56:43 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103, 10.4.0.169, executor 1, partition 103, ANY, 8050 bytes)\\n19/04/23 19:56:43 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 23180 ms on 10.4.0.169 (executor 1) (44/107)\\n19/04/23 19:56:43 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104, 10.4.0.89, executor 3, partition 104, ANY, 8050 bytes)\\n19/04/23 19:56:43 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 7024 ms on 10.4.0.89 (executor 3) (45/107)\\n19/04/23 19:56:43 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105, 10.4.0.240, executor 6, partition 105, ANY, 8050 bytes)\\n19/04/23 19:56:43 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 23781 ms on 10.4.0.240 (executor 6) (46/107)\\n19/04/23 19:56:43 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106, 10.4.0.193, executor 13, partition 106, ANY, 8033 bytes)\\n19/04/23 19:56:43 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 7580 ms on 10.4.0.193 (executor 13) (47/107)\\n19/04/23 19:56:43 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 23934 ms on 10.4.0.240 (executor 6) (48/107)\\n19/04/23 19:56:43 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 23968 ms on 10.4.0.169 (executor 1) (49/107)\\n19/04/23 19:56:44 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 24193 ms on 10.4.0.55 (executor 8) (50/107)\\n19/04/23 19:56:44 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 13064 ms on 10.4.0.169 (executor 1) (51/107)\\n19/04/23 19:56:44 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 24461 ms on 10.4.0.71 (executor 0) (52/107)\\n19/04/23 19:56:44 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 24671 ms on 10.4.0.18 (executor 12) (53/107)\\n19/04/23 19:56:44 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 24700 ms on 10.4.0.51 (executor 4) (54/107)\\n19/04/23 19:56:45 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 25562 ms on 10.4.0.193 (executor 13) (55/107)\\n19/04/23 19:56:45 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 25885 ms on 10.4.0.141 (executor 9) (56/107)\\n19/04/23 19:56:45 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 25876 ms on 10.4.0.12 (executor 11) (57/107)\\n19/04/23 19:56:45 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 26011 ms on 10.4.1.3 (executor 2) (58/107)\\n19/04/23 19:56:46 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 26652 ms on 10.4.0.12 (executor 11) (59/107)\\n19/04/23 19:56:46 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 14157 ms on 10.4.0.71 (executor 0) (60/107)\\n19/04/23 19:56:47 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 8064 ms on 10.4.0.69 (executor 14) (61/107)\\n19/04/23 19:56:47 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 28063 ms on 10.4.0.55 (executor 8) (62/107)\\n19/04/23 19:56:47 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 28134 ms on 10.4.0.97 (executor 10) (63/107)\\n19/04/23 19:56:47 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 10976 ms on 10.4.0.69 (executor 14) (64/107)\\n19/04/23 19:56:48 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 12267 ms on 10.4.0.89 (executor 3) (65/107)\\n19/04/23 19:56:49 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 29419 ms on 10.4.0.18 (executor 12) (66/107)\\n19/04/23 19:56:49 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 13758 ms on 10.4.0.69 (executor 14) (67/107)\\n19/04/23 19:56:49 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 29913 ms on 10.4.0.193 (executor 13) (68/107)\\n19/04/23 19:56:49 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 10892 ms on 10.4.1.3 (executor 2) (69/107)\\n19/04/23 19:56:50 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 30530 ms on 10.4.0.55 (executor 8) (70/107)\\n19/04/23 19:56:50 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 9472 ms on 10.4.0.169 (executor 1) (71/107)\\n19/04/23 19:56:50 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 9300 ms on 10.4.0.141 (executor 9) (72/107)\\n19/04/23 19:56:51 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 31486 ms on 10.4.0.240 (executor 6) (73/107)\\n19/04/23 19:56:51 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 31501 ms on 10.4.0.71 (executor 0) (74/107)\\n19/04/23 19:56:51 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 11451 ms on 10.4.0.142 (executor 5) (75/107)\\n19/04/23 19:56:51 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 8725 ms on 10.4.0.51 (executor 4) (76/107)\\n19/04/23 19:56:51 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 31965 ms on 10.4.0.55 (executor 8) (77/107)\\n19/04/23 19:56:52 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 9604 ms on 10.4.0.240 (executor 6) (78/107)\\n19/04/23 19:56:52 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 11543 ms on 10.4.0.51 (executor 4) (79/107)\\n19/04/23 19:56:52 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 12917 ms on 10.4.0.141 (executor 9) (80/107)\\n19/04/23 19:56:53 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 10678 ms on 10.4.0.51 (executor 4) (81/107)\\n19/04/23 19:56:53 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 12418 ms on 10.4.0.141 (executor 9) (82/107)\\n19/04/23 19:56:53 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 9819 ms on 10.4.0.240 (executor 6) (83/107)\\n19/04/23 19:56:53 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 21621 ms on 10.4.0.69 (executor 14) (84/107)\\n19/04/23 19:56:53 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 14165 ms on 10.4.1.3 (executor 2) (85/107)\\n19/04/23 19:56:53 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 13888 ms on 10.4.0.18 (executor 12) (86/107)\\n19/04/23 19:56:53 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 14269 ms on 10.4.0.89 (executor 3) (87/107)\\n19/04/23 19:56:53 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 14182 ms on 10.4.0.18 (executor 12) (88/107)\\n19/04/23 19:56:54 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 10908 ms on 10.4.0.89 (executor 3) (89/107)\\n19/04/23 19:56:54 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 15811 ms on 10.4.0.71 (executor 0) (90/107)\\n19/04/23 19:56:54 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 14335 ms on 10.4.0.142 (executor 5) (91/107)\\n19/04/23 19:56:54 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 10764 ms on 10.4.0.193 (executor 13) (92/107)\\n19/04/23 19:56:54 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 22762 ms on 10.4.0.25 (executor 7) (93/107)\\n19/04/23 19:56:55 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 15550 ms on 10.4.0.25 (executor 7) (94/107)\\n19/04/23 19:56:55 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 12057 ms on 10.4.0.169 (executor 1) (95/107)\\n19/04/23 19:56:55 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 35371 ms on 10.4.0.193 (executor 13) (96/107)\\n19/04/23 19:56:55 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 14979 ms on 10.4.1.3 (executor 2) (97/107)\\n19/04/23 19:56:55 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 13848 ms on 10.4.0.97 (executor 10) (98/107)\\n19/04/23 19:56:55 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 35757 ms on 10.4.0.12 (executor 11) (99/107)\\n19/04/23 19:56:55 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 15139 ms on 10.4.0.12 (executor 11) (100/107)\\n19/04/23 19:56:55 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 14111 ms on 10.4.0.97 (executor 10) (101/107)\\n19/04/23 19:56:55 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 15014 ms on 10.4.0.25 (executor 7) (102/107)\\n19/04/23 19:56:55 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 14863 ms on 10.4.0.142 (executor 5) (103/107)\\n19/04/23 19:56:56 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 13434 ms on 10.4.0.97 (executor 10) (104/107)\\n19/04/23 19:56:56 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 16578 ms on 10.4.0.89 (executor 3) (105/107)\\n19/04/23 19:56:57 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 14685 ms on 10.4.0.142 (executor 5) (106/107)\\n19/04/23 19:56:58 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 16032 ms on 10.4.0.25 (executor 7) (107/107)\\n19/04/23 19:56:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \\n19/04/23 19:56:58 INFO DAGScheduler: ResultStage 0 (save at NativeMethodAccessorImpl.java:0) finished in 39.187 s\\n19/04/23 19:56:58 INFO DAGScheduler: Job 0 finished: save at NativeMethodAccessorImpl.java:0, took 39.275304 s\\n19/04/23 19:56:58 INFO FileFormatWriter: Write Job f570bce2-9916-4f58-86b6-43b54811bd5a committed.\\n19/04/23 19:56:58 INFO FileFormatWriter: Finished processing stats for write job f570bce2-9916-4f58-86b6-43b54811bd5a.\\n19/04/23 19:56:58 INFO SparkUI: Stopped Spark web UI at http://node-master:4040\\n19/04/23 19:56:58 INFO StandaloneSchedulerBackend: Shutting down all executors\\n19/04/23 19:56:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\\norg.apache.spark.SparkException: Could not find AppClient.\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)\\n\\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)\\n\\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)\\n\\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)\\n\\tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n19/04/23 19:56:59 INFO MemoryStore: MemoryStore cleared\\n19/04/23 19:56:59 INFO BlockManager: BlockManager stopped\\n19/04/23 19:56:59 INFO BlockManagerMaster: BlockManagerMaster stopped\\n19/04/23 19:56:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\\n19/04/23 19:56:59 INFO SparkContext: Successfully stopped SparkContext\\n19/04/23 19:56:59 INFO ShutdownHookManager: Shutdown hook called\\n19/04/23 19:56:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd94b948-4036-4915-9615-8b0bb0dc3106\\n19/04/23 19:56:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-6e79d75d-52b4-4095-97d0-0958e8ef7d86\\n19/04/23 19:56:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-6e79d75d-52b4-4095-97d0-0958e8ef7d86/pyspark-76fa19ac-9e78-4680-93aa-e0c81b139468\\n\",\"job-state\":\"finished\"}}}\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "import requests\n",
    "DATAPROCESSOR_URL = \"http://localhost:4000\"\n",
    "headers = {'Content-type': 'application/vnd.api+json'}\n",
    "\n",
    "response = requests.post(DATAPROCESSOR_URL+\"/api/processing_jobs/5/run\", headers=headers)\n",
    "print(response.text)\n",
    "\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197.69988322257996\n"
     ]
    }
   ],
   "source": [
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "197.699s vs 190.53230738639832s\n",
    "\n",
    "So, the overhead of sending the response from Cmd to Elixir and adding it to the response request looks like 7 secs slower. It is a lot but, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. DataProcessor Solution\n",
    "\n",
    "For DataProcessor, our approach will be different. Since it is just a webserver that handle REST requests, we will show you only the REST requests and the spent time on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Preparing the data\n",
    "\n",
    "Now we use some data munging to derive data that will be useful right next:\n",
    "- First, we will map data related to the same trip to be visualized as a tuple of origin and destination - trips that only have a single point are cut off.\n",
    "- Then, we will round the tick in windows of 20 minutes (so that a trip ocurring at 08:21 and at 08:23 are paired).\n",
    "- To facilitate the whole process, we cast the tick to a timestamp, so that Spark can visualize it as datetime.\n",
    "- To speed things up, at the end we make cache of the data\n",
    "- To be able to make SQL queries in Spark we create a Spark view (similar to a SQL table) with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_jvm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c53fae3a563a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uuid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"T0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m leadedDf = (df\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/sql/window.py\u001b[0m in \u001b[0;36mpartitionBy\u001b[0;34m(*cols)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \"\"\"\n\u001b[1;32m     71\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mjspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_java_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mWindowSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jvm'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead, col, hour, minute\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.partitionBy(\"uuid\").orderBy(\"T0\")\n",
    "\n",
    "leadedDf = (df\n",
    "    .withColumn(\"T1\", lead(col(\"T0\"), 1).over(w))\n",
    "    .withColumn(\"(T1-T0)\", col(\"T1\") - col(\"T0\"))\n",
    "    .withColumn(\"round(T0)\", (col(\"T0\")/1800).cast(\"int\")*1800)\n",
    "    .withColumn(\"V\", lead(col(\"U\"), 1).over(w))\n",
    "    .filter(\"`T1` is not null\")\n",
    "    .withColumn(\"TWindow\", col(\"T0\").cast(\"timestamp\"))\n",
    "    .withColumn(\"H\", hour(\"TWindow\"))\n",
    "    .withColumn(\"M\", ((minute(\"TWindow\")/20).cast(\"int\")*20)))\n",
    "\n",
    "leadedDf.createOrReplaceGlobalTempView(\"traffic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Validation\n",
    "\n",
    "As we said before, now we just want to make sure that the simulation data looks right. So we will just group the data using time windows and analyse the results.\n",
    "\n",
    "We will group the data using Spark instead of Pandas, since Spark can use the whole cluster to process the data, achieving better performance.\n",
    "\n",
    "After processing the data with Spark we finally turn it into a Pandas dataframe, since Pandas has easy-to-use plot functions, while Spark do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficDf = (spark\n",
    "             .sql(\"select round(T0, 0) from global_temp.traffic\")\n",
    "             .groupby(\"round(T0, 0)\")\n",
    "             .count())\n",
    "\n",
    "pandasDf = trafficDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "pandasDf['current_time'] = pd.to_datetime(pandasDf['round(T0, 0)'], unit='s').dt.strftime('%H:%M')\n",
    "\n",
    "pandasDf.sort_values('round(T0, 0)').plot(x='current_time', y='count', kind='bar', figsize=(18, 16))\n",
    "from matplotlib import pyplot as plt\n",
    "plt.savefig('foo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks like the majority of trips started around 07AM and 18PM, which makes a lot of sense. The only weird thing is that 00:00 and 00:30 are presented two times, which does not makes sense. The reason why this happened is because we ignored the day; one of these data at 0:00 and other at 0:30 are actually from the next day, so that they are not grouped together. Now we should check the durations (i.e: difference between ticks T0 and T1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first, last, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticksDiffDf = (spark\n",
    "             .sql(\"select uuid,T0,T1,round(T0) from global_temp.traffic\")\n",
    "             .withColumn(\"(T1-T0)\", col(\"T1\") - col(\"T0\"))\n",
    "             .groupby(\"uuid\").agg(sum(col(\"(T1-T0)\")), min(col(\"T0\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "newDf = (ticksDiffDf\n",
    "    .groupby(\"uuid\")\n",
    "    .agg(max(col(\"Tf\")), first(col(\"rounded_T0\"))))\n",
    "\n",
    "newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdTicksDf = newDf.toPandas()\n",
    "\n",
    "pdTicksDf['current_time'] = pd.to_datetime(pdTicksDf['first(rounded_T0, false)'], unit='s').dt.strftime('%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdTicksDf.sort_values('current_time').plot(x='first(rounded_T0, false)', y='rounded_T0', kind='bar', figsize=(18, 16))\n",
    "from matplotlib import pyplot as plt\n",
    "plt.savefig('foo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdTicksDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pdTicksDf[\"avg((T1-T0))\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from global_temp.traffic\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Defining which edges to remove from city graph\n",
    "\n",
    "We will just count the most used edges and remove them from our graph. First,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, mean\n",
    "\n",
    "df = (spark\n",
    "      .sql(\"select U,V,(T1 - T0) from global_temp.traffic\")\n",
    "      .groupby(\"U\", \"V\")\n",
    "      .agg(count(col(\"(T1 - T0)\")), mean(col(\"(T1 - T0)\")))\n",
    "      .orderBy(col(\"count((T1 - T0))\").desc())\n",
    "      .limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "pdDf[\"coords\"] = list(zip(pdDf[\"U\"], pdDf[\"V\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(18,16)) \n",
    "f = sns.distplot(toPlotDf['avg((T1 - T0))'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, although it looks like the edges that we are going to remove has high vehicle concentration, it has a decent flow (30secs to successfully pass an edge). We are going to take this in consideration at the final analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import stddev_pop, mean\n",
    "\n",
    "df = (spark\n",
    "      .sql(\"select U,V,(T1 - T0),round(T0, 0) from global_temp.traffic\")\n",
    "      .groupby(\"U\", \"V\", \"round(T0, 0)\")\n",
    "      .agg(mean(\"(T1 - T0)\"), stddev_pop(\"(T1 - T0)\"))\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(col(\"stddev_pop((T1 - T0))\").desc()).show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"stddev_samp((T1 - T0))\").describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
